{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import hyperlink\n",
    "import docx\n",
    "from docx import Document\n",
    "import dryscrape\n",
    "import requests_html\n",
    "from requests_html import HTMLSession\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_job_search(num_pages, position):\n",
    "    \n",
    "    if position == 'da':    \n",
    "        url = 'https://il.indeed.com/jobs?q=data+analyst&l=%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91+-%D7%99%D7%A4%D7%95%2C+%D7%9E%D7%97%D7%95%D7%96+%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91&ts=1592342093422&pts=1591532833716&rq=1&rsIdx=0'\n",
    "    elif position == 'ds':\n",
    "        url = 'https://il.indeed.com/jobs?q=Data+Scientist&l=%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91+-%D7%99%D7%A4%D7%95%2C+%D7%9E%D7%97%D7%95%D7%96+%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91&ts=1591532964826&rq=1&rsIdx=2&fromage=last&newcount=6'\n",
    "    elif position == 'de':\n",
    "        url = 'https://il.indeed.com/jobs?q=data+engineer&l=%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91+-%D7%99%D7%A4%D7%95%2C+%D7%9E%D7%97%D7%95%D7%96+%D7%AA%D7%9C+%D7%90%D7%91%D7%99%D7%91&ts=1592341157736&pts=1592321207153&rq=1&rsIdx=1'\n",
    "        \n",
    "    job_names = []\n",
    "    job_links = []\n",
    "    \n",
    "    for page in range(1,num_pages):\n",
    "        if page == 1:\n",
    "            url2 = url\n",
    "        elif page > 1:\n",
    "            url2 = url + '&start=' + str((page-1)*10)\n",
    "        \n",
    "        page = requests.get(url2)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for each in soup.find_all('h2'):\n",
    "            job_names.append(each.get_text())\n",
    "            for a in each.find_all('a', href=True):\n",
    "                job_links.append('https://il.indeed.com' + a['href'])\n",
    "    \n",
    "    for i in range(len(job_names)):\n",
    "        job_names[i] = re.sub(r'(\\n)', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'[^a-zA-Z\\s-]', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'^(\\s)+', '', job_names[i])\n",
    "        \n",
    "    #job_names = job_names[re.match('[^(\\\\n)]', job_names)]\n",
    "    \n",
    "    num_results = len(job_names)\n",
    "        \n",
    "    return num_results, job_names, job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writing_to_file(file_path, cur_index, num_results, job_names, job_links):    \n",
    "    \n",
    "    f = open(file_path, 'a')\n",
    "    \n",
    "    for i in range(cur_index, cur_index + num_results + 1):\n",
    "        f.write('{}. {: <30} {: <} \\n'.format(i, job_names[i], job_links[i]))\n",
    "        #f.write(str(i)+' ' + job_names[i] + ' ' + job_links[i] + '\\n')\n",
    "    \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drushim_job_search(num_pages, position):\n",
    "    \n",
    "    if position == 'da':    \n",
    "        url = 'https://www.drushim.co.il/jobs/subcat/581/'\n",
    "    elif position == 'ds':\n",
    "        url = 'https://www.drushim.co.il/jobs/subcat/511/'\n",
    "    elif position == 'de':\n",
    "        url = 'https://www.drushim.co.il/jobs/subcat/582/'\n",
    "        \n",
    "    job_names, job_links = [], []\n",
    "        \n",
    "    for page in range(1,num_pages+1):\n",
    "        if page == 1:\n",
    "            url2 = url\n",
    "        elif page > 1:\n",
    "            url2 = url + '?page=' + str(page-1)\n",
    "        \n",
    "        page = requests.get(url2)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for each in soup.find_all('h2'):\n",
    "            job_names.append(each.get_text()) \n",
    "            job_links.append(url2)\n",
    "    \n",
    "    for i in range(len(job_names)):\n",
    "        job_names[i] = re.sub(r'(\\n)', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'[^a-zA-Z\\s-]', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'^(\\s)+', '', job_names[i])\n",
    "        \n",
    "    num_results = len(job_names)\n",
    "    \n",
    "    return num_results, job_names, job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.jobmaster.co.il/jobs/?q=data+scientist&l=%D7%9E%D7%A8%D7%9B%D7%96'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_asyncio.Future' object has no attribute 'html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a878bdc96033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#r.html.search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_asyncio.Future' object has no attribute 'html'"
     ]
    }
   ],
   "source": [
    "session = AsyncHTMLSession()\n",
    " \n",
    "r = session.get(link)\n",
    " \n",
    "r.html.render()\n",
    " \n",
    "#r.html.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = dryscrape.Session()\n",
    "#session.visit(link)\n",
    "#response = session.body()\n",
    "#soup = BeautifulSoup(response)\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alljobs_job_search(num_pages, position):\n",
    "    \n",
    "    if position == 'da':    \n",
    "        url = 'https://www.alljobs.co.il/SearchResultsGuest.aspx?page=1&position=&type=&freetxt=data%20analyst&city=&region='\n",
    "    elif position == 'ds':\n",
    "        url = 'https://www.alljobs.co.il/SearchResultsGuest.aspx?page=1&position=&type=&freetxt=data%20scientist&city=&region='\n",
    "    elif position == 'de':\n",
    "        url = 'https://www.alljobs.co.il/SearchResultsGuest.aspx?page=1&position=&type=&freetxt=data%20engineer&city=&region='\n",
    "        \n",
    "    job_names = []\n",
    "    job_links = []\n",
    "    \n",
    "    for page in range(1,num_pages):\n",
    "        if page == 1:\n",
    "            url2 = url\n",
    "        elif page > 1:\n",
    "            url2 = url + '&start=' + str((page-1)*10)\n",
    "        \n",
    "        page = requests.get(url2)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for each in soup.find_all('h1'):\n",
    "            job_names.append(each.get_text())\n",
    "            for a in each.find_all('a', href=True):\n",
    "                job_links.append(a['href'])\n",
    "    \n",
    "    for i in range(len(job_names)):\n",
    "        job_names[i] = re.sub(r'(\\n)', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'[^a-zA-Z\\s-]', '', job_names[i])\n",
    "        job_names[i] = re.sub(r'^(\\s)+', '', job_names[i])\n",
    "        \n",
    "    #job_names = job_names[re.match('[^(\\\\n)]', job_names)]\n",
    "    \n",
    "    num_results = len(job_names)\n",
    "        \n",
    "    return num_results, job_names, job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alljobs_job_search(2, 'da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.alljobs.co.il/SearchResultsGuest.aspx?page=1&position=&type=&freetxt=data%20engineer&city=&region='\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "\n",
    "document = docx.Document()\n",
    "p = document.add_paragraph()\n",
    "add_hyperlink(p, 'http://www.google.com', 'Google')\n",
    "document.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://bet.hkjc.com/default.aspx?url=football/odds/odds_allodds.aspx&lang=EN&tmatchid=120998'\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
